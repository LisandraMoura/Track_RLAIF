{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8105a71d",
   "metadata": {},
   "source": [
    "# Modelo de Recompensa ou Reward Model (RM)\n",
    "\n",
    "O modelo de recompensa √© treinado para aprender uma fun√ß√£o escalar que estima o qu√£o boa √© uma resposta \n",
    "ùë¶ para um prompt x, de acordo com um crit√©rio de prefer√™ncia (humano ou de IA).\n",
    "\n",
    "O RM normalmente √© treinado com perguntas e pares de respostas escolhidas e rejeitadas, esse tipo de dataset √© chamado de \"prefer√™ncias\" em que podem ser gerados tanto por humanos como por IA.\n",
    "\n",
    "Normalmente, o RM √© um LLM pr√©-treinado com uma camada linear que transforma o hidden state do √∫ltimo token em um score escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407f26e",
   "metadata": {},
   "source": [
    "# UltraFeedback\n",
    "\n",
    "Como j√° existe um RM treinado pela equipe que desenvolveu o artigo do Ultrafeedback, vamos apenas importar o modelo do Hugging Face para ver como ele funciona.\n",
    "\n",
    "https://huggingface.co/openbmb/UltraRM-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7508eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "\n",
    "class LlamaRewardModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward( # args are the same as LlamaForCausalLM\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "                                input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids,\n",
    "                                past_key_values=past_key_values,\n",
    "                                inputs_embeds=inputs_embeds,                               \n",
    "                            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.regression_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        ends = attention_mask.cumsum(dim=1).argmax(dim=1).view(-1,1)\n",
    "        rewards = torch.gather(rewards, 1, ends)\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "ultrarm_template = \"\"\"Human: {instruction}\n",
    "\n",
    "Assistant: {completion}\"\"\"\n",
    "\n",
    "dataset = [ # cases in webgpt\n",
    "  {\"chosen\":\"Human: \\\"Who orders martinis \\\"\\\"shaken, not stirred\\\"\\\"?\\\"\\nAssistant: Sean Connery's character, fictional British Secret Service agent James Bond, in the movie Goldfinger, stated that he preferred his martini to be \\\"shaken, not stirred\\\". [1] Some believe that Bond ordered his martini shaken because of the vodka it contained, as vodka was, for the most part, refined from potatoes (cheaper brands) which made the vodka oily. To disperse the oil, Bond ordered his martinis shaken. [2]\",\"rejected\":\"Human: \\\"Who orders martinis \\\"\\\"shaken, not stirred\\\"\\\"?\\\"\\nAssistant: Fleming's fictional British Secret Service agent James Bond orders his martini cocktail shaken, not stirred [1]. Bond's preferences for his martini are carried over to the films, where his orders are seen in both the 1961 film Dr. No and the 2006 film Casino Royale [1, 2]. In both films, Bond's subordinates copy his order, telling the bartender to keep the fruit with their drinks [2]. However, in the 2006 film, Bond appears irritated when the bartender asks if he would like his drink shaken or stirred [2].\"},\n",
    "  {\"chosen\":\"Human: Sural relates to which part of the body?\\nAssistant: The sural region is the muscular swelling of the back of the leg below the knee, formed chiefly by the bellies of the gastrocnemius and soleus muscles [1,2].\",\"rejected\":\"Human: Sural relates to which part of the body?\\nAssistant: The Sural nerve runs down the side of the leg near the small saphenous vein, then passes forward below the lateral malleolus and continues on the outside of the foot as the lateral dorsal cutaneous nerve, which then communicates with the intermediate dorsal cutaneous nerve, which branches off to the side of the foot. [1]\"}\n",
    "]\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"openbmb/UltraRM-13b\")\n",
    "model = LlamaRewardModel.from_pretrained(\"openbmb/UltraRM-13b\")\n",
    "model.eval()\n",
    "\n",
    "for example in dataset:\n",
    "    inputs = tokenizer(example[\"chosen\"], return_tensors=\"pt\")\n",
    "    chosen_reward = model(**inputs).item()\n",
    "    inputs = tokenizer(example[\"rejected\"], return_tensors=\"pt\")\n",
    "    rejected_reward = model(**inputs).item()\n",
    "    print(chosen_reward - rejected_reward)\n",
    "\n",
    "# Output 1: 2.4158712085336447\n",
    "# Output 2: 0.1896953582763672\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc02e5",
   "metadata": {},
   "source": [
    "# Sem LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoca 0 | Perda m√©dia: 0.1413\n",
      "√âpoca 20 | Perda m√©dia: 0.0489\n",
      "√âpoca 40 | Perda m√©dia: 0.0266\n",
      "√âpoca 60 | Perda m√©dia: 0.0176\n",
      "√âpoca 80 | Perda m√©dia: 0.0128\n",
      "\n",
      "Pergunta: ('Explique o que √© aprendizado por refor√ßo.', '√â um m√©todo de aprendizado em que um agente aprende por tentativa e erro, recebendo recompensas por boas a√ß√µes.', '√â uma t√©cnica onde o modelo refor√ßa os erros para aprender mais r√°pido.')\n",
      "Score chosen  : 33.5429\n",
      "Score rejected: 25.6767\n",
      "\n",
      "Pergunta: ('O que √© uma rede neural?', '√â um modelo computacional inspirado no c√©rebro humano, usado para reconhecer padr√µes e tomar decis√µes.', '√â uma ferramenta que copia o c√©rebro humano literalmente e pensa sozinha.')\n",
      "Score chosen  : 26.9877\n",
      "Score rejected: 23.0547\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ============================================================\n",
    "# 1. Pequeno \"dataset\" simulado\n",
    "# ============================================================\n",
    "dataset = [\n",
    "    {\n",
    "        \"pergunta\": \"Explique o que √© aprendizado por refor√ßo.\",\n",
    "        \"chosen\":   \"√â um m√©todo de aprendizado em que um agente aprende por tentativa e erro, recebendo recompensas por boas a√ß√µes.\",\n",
    "        \"rejected\": \"√â uma t√©cnica onde o modelo refor√ßa os erros para aprender mais r√°pido.\"\n",
    "    },\n",
    "    {\n",
    "        \"pergunta\": \"O que √© uma rede neural?\",\n",
    "        \"chosen\":   \"√â um modelo computacional inspirado no c√©rebro humano, usado para reconhecer padr√µes e tomar decis√µes.\",\n",
    "        \"rejected\": \"√â uma ferramenta que copia o c√©rebro humano literalmente e pensa sozinha.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 2. Tokeniza√ß√£o bem simples (s√≥ contagem de palavras)\n",
    "# ============================================================\n",
    "def encode_text(text):\n",
    "    return torch.tensor([len(text.split())], dtype=torch.float32)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Modelo de recompensa simples: uma camada linear\n",
    "# ============================================================\n",
    "class SimpleRewardModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # entrada = n√∫mero de tokens, sa√≠da = score\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SimpleRewardModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# ============================================================\n",
    "# 4. Fun√ß√£o de perda tipo \"pairwise ranking loss\"\n",
    "#    (igual usada em RLHF/RLAIF)\n",
    "# ============================================================\n",
    "def pairwise_loss(chosen_reward, rejected_reward):\n",
    "    return -torch.log(torch.sigmoid(chosen_reward - rejected_reward)).mean()\n",
    "\n",
    "# ============================================================\n",
    "# 5. Treinamento\n",
    "# ============================================================\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for item in dataset:\n",
    "        chosen_input = encode_text(item[\"pergunta\"] + \" \" + item[\"chosen\"])\n",
    "        rejected_input = encode_text(item[\"pergunta\"] + \" \" + item[\"rejected\"])\n",
    "\n",
    "        chosen_reward = model(chosen_input)\n",
    "        rejected_reward = model(rejected_input)\n",
    "\n",
    "        loss = pairwise_loss(chosen_reward, rejected_reward)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"√âpoca {epoch} | Perda m√©dia: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. Avalia√ß√£o: ver se o modelo aprendeu a dar maior score ao chosen\n",
    "# ============================================================\n",
    "for item in dataset:\n",
    "    chosen_input = encode_text(item[\"pergunta\"] + \" \" + item[\"chosen\"])\n",
    "    rejected_input = encode_text(item[\"pergunta\"] + \" \" + item[\"rejected\"])\n",
    "    with torch.no_grad():\n",
    "        r_chosen = model(chosen_input).item()\n",
    "        r_rejected = model(rejected_input).item()\n",
    "    print(f\"\\nPergunta: {item['pergunta'], item['chosen'], item['rejected']}\")\n",
    "    print(f\"Score chosen  : {r_chosen:.4f}\")\n",
    "    print(f\"Score rejected: {r_rejected:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
