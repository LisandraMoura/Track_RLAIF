{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309898df",
   "metadata": {},
   "source": [
    "# Funções de recompensa\n",
    "\n",
    "No repositório [Open-R1](https://github.com/huggingface/open-r1/blob/main/src/open_r1/rewards.py) da Hugging Face, que tem como objetivo reproduzir o artigo do DeepSeek-R1, são disponibilizadas diversas funções de recompensa que podem ser utilizadas para tornar o processo de recompensa mais granular.\n",
    "\n",
    "Essas funções permitem avaliar o desempenho do modelo em diferentes níveis de detalhe, conforme o tipo de tarefa ou o comportamento desejado.\n",
    "\n",
    "Além disso, há vários artigos que propõem estratégias específicas de definição de recompensas, dependendo da natureza da atividade.\n",
    "Um exemplo é o artigo [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958), que apresenta uma forma particular de atribuir recompensas em cenários envolvendo o uso de ferramentas (tool use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a86825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo Correto:\n",
      "1.0\n",
      "Exemplo Errado:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Exemplo inspirado no Open-R1: função simples de acurácia\n",
    "# Nesse caso, calcula-se a acurácia comparando a resposta gerada pelo modelo com uma resposta de referência.\n",
    "# Suponha que temos uma resposta esperada (referência) e a saída gerada pelo modelo.\n",
    "# Por exemplo, imagine que você está ajustando um modelo para gerar código em Python e quer recompensá-lo\n",
    "# quando ele acerta exatamente a resposta esperada.\n",
    "\n",
    "def accuracy(predictions, references):\n",
    "    correct = 0\n",
    "    total = len(references)\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        if pred == ref:\n",
    "            correct += 1\n",
    "            \n",
    "    return print(correct / total if total > 0 else 0)\n",
    "\n",
    "# Primeiro exemplo\n",
    "predictions = [\"import pandas as pd # This is a coding in python for example\"]\n",
    "references = [\"import pandas as pd # This is a coding in python for example\"]\n",
    "print(\"Exemplo Correto:\")\n",
    "acc = accuracy(predictions, references)\n",
    "\n",
    "predictions = [\"import numpy as np # This is a coding in python for example\"]\n",
    "references = [\"This is a coding in python for example\"]\n",
    "print(\"Exemplo Errado:\")\n",
    "acc = accuracy(predictions, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414f508",
   "metadata": {},
   "source": [
    "# Breve análise das funções de recompensas\n",
    "\n",
    "Analisei os sistemas de recompensa do **OpenR1** e do **ToolRL**, comparando suas abordagens no treinamento de modelos de linguagem com RL.\n",
    "\n",
    "No **OpenR1**, estudei o algoritmo **GRPO** (uma variante do PPO) e suas múltiplas funções de recompensa, como:\n",
    "\n",
    "- **accuracy_reward** (validação matemática via Math-Verify),\n",
    "- **format_reward** (checagem de estrutura),\n",
    "- **reasoning_steps_reward** (incentivo a raciocínios passo a passo),\n",
    "- **code_reward** (execução de código em sandbox), entre outras.\n",
    "- O sistema é altamente granular, avaliando desde clareza textual até penalidades por repetição ou respostas muito longas.\n",
    "\n",
    "Já no **ToolRL**, as recompensas são mais diretas, combinando **R_format** (validação de sintaxe, como tags `<think>`) e **R_correct** (correspondência com o *ground-truth*). Apesar da simplicidade, a estrutura é eficaz para tarefas baseadas em ferramentas.\n",
    "\n",
    "Concluí que o OpenR1 oferece um controle mais refinado para modelos de raciocínio (como matemática), enquanto o ToolRL prioriza eficiência em chamadas de API/tools. Ambos podem ser complementares, dependendo do objetivo de treinamento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
